{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py3-TF2.0]",
      "language": "python",
      "name": "conda-env-py3-TF2.0-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "DeepLearning_Preprocessing 12.5.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOUhSwmMYgbE"
      },
      "source": [
        "# Audiobooks business case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ob3Lr_cYgbH"
      },
      "source": [
        "## Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)\n",
        "\n",
        "There are three main steps for preprocessing the dataset:\n",
        "\n",
        "\n",
        "*   Balance the dataset\n",
        "*   Split the dataset into training, validation and test\n",
        "*   Save the dataset as Tensor friendly format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note that we have removed the header row, which contains the names of the categories. We simply want the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcplaQ1YgbH"
      },
      "source": [
        "### Extract the data from the csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQrQF8IqYgbI"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Df0w1ha2RG",
        "outputId": "2fd54708-b5e2-4634-e0bc-9a1b8961a637"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XZo-tp6azoI"
      },
      "source": [
        "raw_csv_data = np.loadtxt('/content/drive/My Drive/Customer Analytics/Audiobooks_data.csv', delimiter=',')\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
        "targets_all = raw_csv_data[:,-1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvrf1OcsYgbJ"
      },
      "source": [
        "### Balance the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew6BC5gpYgbJ"
      },
      "source": [
        "#Here we balance the dataset manually instead of using Python packages.\n",
        "\n",
        "# Count the number of targets that are 0 and number of targets that are 1. \n",
        "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
        "num_one_targets = int(np.sum(targets_all))\n",
        "\n",
        "zero_targets_counter = 0\n",
        "\n",
        "indices_to_remove = []\n",
        "\n",
        "for i in range(targets_all.shape[0]):\n",
        "    if targets_all[i] == 0:\n",
        "        zero_targets_counter += 1\n",
        "        if zero_targets_counter > num_one_targets:\n",
        "            indices_to_remove.append(i)\n",
        "#Delete data that marked remove in the loop.\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4lAhx4tYgbJ"
      },
      "source": [
        "### Standardize the inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSXYVOO6YgbJ"
      },
      "source": [
        "#Create standard scaler. Transform the inputs\n",
        "scaler_deep_learning = StandardScaler()\n",
        "scaled_inputs = scaler_deep_learning.fit_transform(unscaled_inputs_equal_priors)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XScue1ESYgbK"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFk95_0PYgbK"
      },
      "source": [
        "#Shuffle the data so the data would be randomly spread out\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUk0NxdBYgbK"
      },
      "source": [
        "### Split the dataset into train, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84u3iyUtYgbL",
        "outputId": "7148b719-826b-4182-dd18-7299144a0adb"
      },
      "source": [
        "#Split the dataset 80-10-10 as training, validation and test datasets.\n",
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "#Check if the three datasets are balanced.\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1801.0 3579 0.5032131880413523\n",
            "211.0 447 0.4720357941834452\n",
            "225.0 448 0.5022321428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoh8gZKLYgbM"
      },
      "source": [
        "### Save the three datasets in *.npz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiqWJPCyYgbM"
      },
      "source": [
        "#Save the datasets in npz file.\n",
        "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7RULA8LYgbM"
      },
      "source": [
        "### Save the scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQPytfgoYgbM"
      },
      "source": [
        "#Save the scaler to apply in new data.\n",
        "pickle.dump(scaler_deep_learning, open('scaler_deep_learning.pickle', 'wb'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OttYz5UCdDg0"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}